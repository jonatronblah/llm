{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20d128c-3165-4aa1-8e84-c7e6a7d01a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from llama_index.core.node_parser import HTMLNodeParser\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import psycopg \n",
    "from llama_index.core import PromptTemplate\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c611779e-d2b6-49d9-99b8-5da8b2c8d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"/setup/on.env\")\n",
    "pg_user = os.getenv(\"POSTGRES_USER\")\n",
    "pg_db = os.getenv(\"POSTGRES_DB\")\n",
    "pg_pwd = os.getenv(\"POSTGRES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffb32d9-8777-4f89-b8a1-00b09bbba951",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B-Instruct\")\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name = \"BAAI/bge-base-en-v1.5\"\n",
    ")\n",
    "\n",
    "set_global_tokenizer(tokenizer.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87af5b7-a077-419a-becc-f4519a35defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/notebooks/data/ttlg-html\"\n",
    "tags = []\n",
    "html_docs = []\n",
    "for ext in [\"*.html\"]:\n",
    "    for path in Path(data_dir).rglob(ext):\n",
    "        with open(path, \"rb\") as file:\n",
    "            html_text = file.read().decode(\"windows-1252\")\n",
    "            soup = BeautifulSoup(html_text)\n",
    "            tags.extend([tag.name for tag in soup.find_all()])\n",
    "            html_docs.append(Document(text=html_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d3f4e5-24d1-4de7-8136-e6bb8ccb7ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(html_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9cc5420-1761-4b19-bdd4-7bd391d2d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "# tags = [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\", \"blockquote\", 'pagetitle']\n",
    "tags = [\"blockquote\", 'pagetitle']\n",
    "\n",
    "parser = HTMLNodeParser(tags=tags)\n",
    "nodes = parser.get_nodes_from_documents(html_docs)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c7f1eae-89ac-4796-9caa-24324e555bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grover'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02edc31d-e579-4ad8-a3e2-ae2e7cb590c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(name):\n",
    "    with psycopg.connect(\n",
    "        f\"host=postgres dbname={pg_db} user={pg_user} password={pg_pwd}\"\n",
    "    ) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"\"\"\n",
    "                drop table if exists {name};\n",
    "                \"\"\")\n",
    "            conn.commit()\n",
    "\n",
    "\n",
    "drop(\"data_html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ff3eac-67e0-4ca5-8e0e-e3c99341750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PGVectorStore.from_params(\n",
    "    database=pg_db,\n",
    "    host=\"postgres\",\n",
    "    password=pg_pwd,\n",
    "    port=5432,\n",
    "    user=pg_user,\n",
    "    table_name=\"html\",\n",
    "    embed_dim=768,\n",
    "    hnsw_kwargs={\n",
    "        \"hnsw_m\": 14,\n",
    "        \"hnsw_ef_construction\": 72,\n",
    "        \"hnsw_ef_search\": 52,\n",
    "        \"hnsw_dist_method\": \"vector_cosine_ops\",\n",
    "    },\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "819912f0-6765-431e-b24e-c0172b02a4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ebce6bc9a04834bb9bced6779bcfc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex(nodes, storage_context=storage_context, show_progress=True, embed_model=Settings.embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e56d68-8269-4a7b-9be2-cbb738f12c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4060 Ti) - 14449 MiB free\n",
      "llama_model_loader: loaded meta data with 38 key-value pairs and 579 tensors from /llamaindex_cache/models/Qwen2.5-14B-Instruct-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 14B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 14B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-1...\n",
      "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 14B\n",
      "llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-14B\n",
      "llama_model_loader: - kv  12:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
      "llama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 48\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                      quantize.imatrix.file str              = /models_out/Qwen2.5-14B-Instruct-GGUF...\n",
      "llama_model_loader: - kv  35:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  36:             quantize.imatrix.entries_count i32              = 336\n",
      "llama_model_loader: - kv  37:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  241 tensors\n",
      "llama_model_loader: - type q6_K:  338 tensors\n",
      "llm_load_vocab: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 48\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 5\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 14B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 14.77 B\n",
      "llm_load_print_meta: model size       = 11.29 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2.5 14B Instruct\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q6_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 48 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 49/49 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size = 10948.23 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   609.08 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 12384\n",
      "llama_new_context_with_model: n_ctx_per_seq = 12384\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 1000000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (12384) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  2322.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2322.00 MiB, K (f16): 1161.00 MiB, V (f16): 1161.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  1031.69 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    34.19 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1686\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.entries_count': '336', 'quantize.imatrix.file': '/models_out/Qwen2.5-14B-Instruct-GGUF/Qwen2.5-14B-Instruct.imatrix', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-14B', 'general.license': 'apache-2.0', 'qwen2.attention.head_count_kv': '8', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'general.type': 'model', 'qwen2.block_count': '48', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.base_model.0.name': 'Qwen2.5 14B', 'general.license.link': 'https://huggingface.co/Qwen/Qwen2.5-14B-Instruct/blob/main/LICENSE', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Qwen', 'general.size_label': '14B', 'tokenizer.ggml.add_bos_token': 'false', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '5120', 'tokenizer.ggml.padding_token_id': '151643', 'general.architecture': 'qwen2', 'qwen2.context_length': '32768', 'qwen2.feed_forward_length': '13824', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'qwen2.attention.head_count': '40', 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'Instruct', 'general.file_type': '18', 'general.name': 'Qwen2.5 14B Instruct', 'tokenizer.ggml.bos_token_id': '151643'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "def completion_to_prompt(completion):\n",
    "   return f\"<|im_start|>system\\n<|im_end|>\\n<|im_start|>user\\n{completion}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|im_start|>system\\n{message.content}<|im_end|>\\n\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|im_start|>user\\n{message.content}<|im_end|>\\n\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|im_start|>assistant\\n{message.content}<|im_end|>\\n\"\n",
    "\n",
    "    if not prompt.startswith(\"<|im_start|>system\"):\n",
    "        prompt = \"<|im_start|>system\\n\" + prompt\n",
    "        \n",
    "\n",
    "    prompt = prompt + '<|im_start|>\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. You answer user queries about the Through The Looking Glass (TTLG) forums using retreived html data scraped from the boards.\\n'\n",
    "\n",
    "    return prompt\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url=\"https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q6_K.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=1024,\n",
    "    context_window=12384,\n",
    "    generate_kwargs={\"repeat_penalty\": 1.15, \"top_k\": 0, \"top_p\": 0.5, \"min_p\": 0.1},\n",
    "    model_kwargs={\n",
    "        \"n_gpu_layers\": -1,\n",
    "    },\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6327173c-191a-4cff-8ca1-12fed461b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 8 prefix-match hit, remaining 1501 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19933.47 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1501 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   202 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   14776.14 ms /  1703 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it seems that one of the popular discussion topics in TTLG forums is whether or not to create a new forum for Arkane's upcoming game \"The Crossing\". There are differing opinions among members about this. Some believe that since TTLG has followed other games from Arkane such as Arx and DoMM, they should include The Crossing too, especially given the involvement of ex-LGSers and ex-IonStorm people in its development. Others argue against it due to lack of information on what the game will be like at this point.\n",
      "\n",
      "Another topic that seems popular is feedback about forum organization and management decisions made by admins. There's a discussion around whether certain forums are necessary or not, with some members suggesting re-organization into sub-forums for companies employing former LG staff as an alternative to adding new individual game forums each time.\n",
      "\n",
      "Lastly, there appears to be interest in the development of The Crossing itself, with forum users expressing excitement about potential news and updates from Arkane.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    index.as_query_engine().query(\n",
    "        'Using the provided context, answer the following query: Provide a summary of popular discussion topics in the TTLG forums.'\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
